{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy import *\n",
    "import scipy.sparse as sparse\n",
    "import numpy as np\n",
    "from scipy.sparse.linalg import spsolve\n",
    "from sklearn.preprocessing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import implicit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "PLIDs = load(\"all_45k_playlist_ordered_by_ID.npy\")\n",
    "test = sparse.load_npz(\"all_playlists_with_tracks.npz\")\n",
    "TIDs = load(\"all_100k_tracks_ordered_by_ID.npy\")\n",
    "\n",
    "PLIDs = ravel(PLIDs)[0].getcol(0).data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def implicit_weighted_ALS(training_set, lambda_val = 0.1, alpha = 40, iterations = 15, rank_size = 30, seed = 0):\n",
    "    '''\n",
    "    Implicit weighted ALS taken from Hu, Koren, and Volinsky 2008. Designed for alternating least squares and implicit\n",
    "    feedback based collaborative filtering. \n",
    "    \n",
    "    parameters:\n",
    "    \n",
    "    training_set - Our matrix of ratings with shape m x n, where m is the number of users and n is the number of items.\n",
    "    Should be a sparse csr matrix to save space. \n",
    "    \n",
    "    lambda_val - Used for regularization during alternating least squares. Increasing this value may increase bias\n",
    "    but decrease variance. Default is 0.1. \n",
    "    \n",
    "    alpha - The parameter associated with the confidence matrix discussed in the paper, where Cui = 1 + alpha*Rui. \n",
    "    The paper found a default of 40 most effective. Decreasing this will decrease the variability in confidence between\n",
    "    various ratings.\n",
    "    \n",
    "    iterations - The number of times to alternate between both user feature vector and item feature vector in\n",
    "    alternating least squares. More iterations will allow better convergence at the cost of increased computation. \n",
    "    The authors found 10 iterations was sufficient, but more may be required to converge. \n",
    "    \n",
    "    rank_size - The number of latent features in the user/item feature vectors. The paper recommends varying this \n",
    "    between 20-200. Increasing the number of features may overfit but could reduce bias. \n",
    "    \n",
    "    seed - Set the seed for reproducible results\n",
    "    \n",
    "    returns:\n",
    "    \n",
    "    The feature vectors for users and items. The dot product of these feature vectors should give you the expected \n",
    "    \"rating\" at each point in your original matrix. \n",
    "    '''\n",
    "    \n",
    "    # first set up our confidence matrix\n",
    "    \n",
    "    conf = (alpha*training_set) # To allow the matrix to stay sparse, I will add one later when each row is taken \n",
    "                                # and converted to dense. \n",
    "    num_user = conf.shape[0]\n",
    "    num_item = conf.shape[1] # Get the size of our original ratings matrix, m x n\n",
    "    \n",
    "    # initialize our X/Y feature vectors randomly with a set seed\n",
    "    rstate = np.random.RandomState(seed)\n",
    "    \n",
    "    X = sparse.csr_matrix(rstate.normal(size = (num_user, rank_size))) # Random numbers in a m x rank shape\n",
    "    Y = sparse.csr_matrix(rstate.normal(size = (num_item, rank_size))) # Normally this would be rank x n but we can \n",
    "                                                                 # transpose at the end. Makes calculation more simple.\n",
    "    X_eye = sparse.eye(num_user)\n",
    "    Y_eye = sparse.eye(num_item)\n",
    "    lambda_eye = lambda_val * sparse.eye(rank_size) # Our regularization term lambda*I. \n",
    "    \n",
    "    # We can compute this before iteration starts. \n",
    "    \n",
    "    # Begin iterations\n",
    "   \n",
    "    for iter_step in range(iterations): # Iterate back and forth between solving X given fixed Y and vice versa\n",
    "        # Compute yTy and xTx at beginning of each iteration to save computing time\n",
    "        yTy = Y.T.dot(Y)\n",
    "        xTx = X.T.dot(X)\n",
    "        # Being iteration to solve for X based on fixed Y\n",
    "        for u in range(num_user):\n",
    "            conf_samp = conf[u,:].toarray() # Grab user row from confidence matrix and convert to dense\n",
    "            pref = conf_samp.copy() \n",
    "            pref[pref != 0] = 1 # Create binarized preference vector \n",
    "            CuI = sparse.diags(conf_samp, [0]) # Get Cu - I term, don't need to subtract 1 since we never added it \n",
    "            yTCuIY = Y.T.dot(CuI).dot(Y) # This is the yT(Cu-I)Y term \n",
    "            yTCupu = Y.T.dot(CuI + Y_eye).dot(pref.T) # This is the yTCuPu term, where we add the eye back in\n",
    "                                                      # Cu - I + I = Cu\n",
    "            X[u] = spsolve(yTy + yTCuIY + lambda_eye, yTCupu) \n",
    "            # Solve for Xu = ((yTy + yT(Cu-I)Y + lambda*I)^-1)yTCuPu, equation 4 from the paper  \n",
    "        # Begin iteration to solve for Y based on fixed X \n",
    "        for i in range(num_item):\n",
    "            conf_samp = conf[:,i].T.toarray() # transpose to get it in row format and convert to dense\n",
    "            pref = conf_samp.copy()\n",
    "            pref[pref != 0] = 1 # Create binarized preference vector\n",
    "            CiI = sparse.diags(conf_samp, [0]) # Get Ci - I term, don't need to subtract 1 since we never added it\n",
    "            xTCiIX = X.T.dot(CiI).dot(X) # This is the xT(Cu-I)X term\n",
    "            xTCiPi = X.T.dot(CiI + X_eye).dot(pref.T) # This is the xTCiPi term\n",
    "            Y[i] = spsolve(xTx + xTCiIX + lambda_eye, xTCiPi)\n",
    "            # Solve for Yi = ((xTx + xT(Cu-I)X) + lambda*I)^-1)xTCiPi, equation 5 from the paper\n",
    "    # End iterations\n",
    "    return X, Y.T # Transpose at the end to make up for not being transposed at the beginning. \n",
    "                         # Y needs to be rank x n. Keep these as separate matrices for scale reasons. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "urm = sparse.load_npz(\"all_playlist_with_tracks_URM.npz\")\n",
    "\n",
    "urm_train = urm[invert(isin(PLIDs, target_playlists))]\n",
    "urm_test = urm[isin(PLIDs, target_playlists)]\n",
    "urm_test = urm_test[:, isin(TIDs, target_tracks_ordered)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<35649x100000 sparse matrix of type '<class 'numpy.int32'>'\n",
       "\twith 677861 stored elements in Compressed Sparse Column format>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urm_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:implicit:This method is deprecated. Please use the AlternatingLeastSquares class instead\n",
      "WARNING:root:OpenBLAS detected. Its highly recommend to set the environment variable 'export OPENBLAS_NUM_THREADS=1' to disable its internal multithreading\n",
      "WARNING:implicit:This method is deprecated. Please use the AlternatingLeastSquares class instead\n",
      "WARNING:root:OpenBLAS detected. Its highly recommend to set the environment variable 'export OPENBLAS_NUM_THREADS=1' to disable its internal multithreading\n"
     ]
    }
   ],
   "source": [
    "alpha = 15\n",
    "user_vecs, item_vecs = implicit.alternating_least_squares((urm_train*alpha).astype('double'), \n",
    "                                                          factors=30, \n",
    "                                                          regularization = 0.1, \n",
    "                                                         iterations = 50)\n",
    "\n",
    "alpha = 15\n",
    "user_vecs2, item_vecs2 = implicit.alternating_least_squares((urm_test*alpha).astype('double'), \n",
    "                                                          factors=30, \n",
    "                                                          regularization = 0.1, \n",
    "                                                         iterations = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl_test = user_vecs[isin(PLIDs, target_playlists)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "monoArtist=load(\"target_playlist_mono_artist.npy\")\n",
    "monoArtistPlID=monoArtist[:,0]\n",
    "monoArtistArtistID=monoArtist[:,1]\n",
    "artistIndexes=load(\"uniqueArtists_NeededToIndexThe_ArtistReducedMatrices.npy\")\n",
    "artistTracks=sparse.load_npz(\"artists_with_tracksID_ordered_by_occurrencies.npz\")\n",
    "target_playlists=genfromtxt(\"target_playlists.csv\",skip_header=1)\n",
    "playlists_with_tracks=load(\"playlists_with_tracks.npy\")\n",
    "target_tracks_ordered = load(\"targetTracksOrdered.npy\")\n",
    "def getsimil(pls, similrow, n):\n",
    "    maxi = flip(argsort(similrow), axis=0)\n",
    "    r = []\n",
    "    for m in maxi:\n",
    "        if(not isin(target_tracks_ordered[m], pls[1:])):\n",
    "           r.append(target_tracks_ordered[m])\n",
    "           if(len(r)==n):\n",
    "               return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_test = item_vecs[isin(TIDs, target_tracks_ordered)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim = user_vecs2.dot(item_test.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute(fname, finalPlSim):\n",
    "    with open(fname,\"a\") as myfile:\n",
    "        myfile.write(\"playlist_id,track_ids\\n\")\n",
    "        for pl, similsum in zip(playlists_with_tracks, finalPlSim):\n",
    "            plID=pl[0]\n",
    "            s = str(int(plID))\n",
    "            s += \",\"\n",
    "            if(isin(plID,monoArtistPlID)):#guarda se c'è un artista solo\n",
    "                artist=monoArtistArtistID[where(monoArtistPlID==plID)[0][0]]#l'unico artista della playlist\n",
    "                artistIndex=where(artistIndexes==artist)[0][0]#cerca la sua posizione nella matrice delle tracce\n",
    "                thisArtistTracks=artistTracks.getrow(artistIndex).data#prende le sue tracce\n",
    "                rr=array([t for t in thisArtistTracks if t not in pl[1:] and isin(t, target_tracks_ordered)])#prende tutte quelle non già presenti\n",
    "\n",
    "                if(len(rr)>=5):\n",
    "                    r = array(rr).take(range(5))\n",
    "                else:\n",
    "                    r4 = getsimil(pl, ravel(similsum), 5)\n",
    "                    r4 = array([el for el in r4 if el not in rr])\n",
    "                    r = concatenate((rr, r4.take(range(5-len(rr)))))\n",
    "            else:\n",
    "                r = getsimil(pl, ravel(similsum), 5)#non c'è un artista solo quindi guarda solo i tag\n",
    "            for el in r:\n",
    "                s+=str(el)\n",
    "                s+=\" \"\n",
    "            myfile.write(s + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "PlwithArtist = sparse.load_npz(\"targetPlaylistArtistReduced.npz\")\n",
    "PlwithArtist = normalize(PlwithArtist, norm=\"max\")\n",
    "tTracksWithArtist = sparse.load_npz(\"targetTracksArtistReduced.npz\")\n",
    "PlTrackSimByArtist = PlwithArtist * tTracksWithArtist.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fin = sim + 0.75*PlTrackSimByArtist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute(\"ALSonlytrain.csv\", fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
